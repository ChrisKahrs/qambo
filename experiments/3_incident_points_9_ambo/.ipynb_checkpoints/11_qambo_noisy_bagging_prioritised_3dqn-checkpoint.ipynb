{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nosy Bagging Duelling Prioritised Replay Double Deep Q Learning - A simple ambulance dispatch point allocation model\n",
    "\n",
    "## Reinforcement learning introduction\n",
    "\n",
    "### RL involves:\n",
    "* Trial and error search\n",
    "* Receiving and maximising reward (often delayed)\n",
    "* Linking state -> action -> reward\n",
    "* Must be able to sense something of their environment\n",
    "* Involves uncertainty in sensing and linking action to reward\n",
    "* Learning -> improved choice of actions over time\n",
    "* All models find a way to balance best predicted action vs. exploration\n",
    "\n",
    "### Elements of RL\n",
    "* *Environment*: all observable and unobservable information relevant to us\n",
    "* *Observation*: sensing the environment\n",
    "* *State*: the perceived (or perceivable) environment \n",
    "* *Agent*: senses environment, decides on action, receives and monitors rewards\n",
    "* *Action*: may be discrete (e.g. turn left) or continuous (accelerator pedal)\n",
    "* *Policy* (how to link state to action; often based on probabilities)\n",
    "* *Reward signal*: aim is to accumulate maximum reward over time\n",
    "* *Value function* of a state: prediction of likely/possible long-term reward\n",
    "* *Q*: prediction of likely/possible long-term reward of an *action*\n",
    "* *Advantage*: The difference in Q between actions in a given state (sums to zero for all actions)\n",
    "* *Model* (optional): a simulation of the environment\n",
    "\n",
    "### Types of model\n",
    "\n",
    "* *Model-based*: have model of environment (e.g. a board game)\n",
    "* *Model-free*: used when environment not fully known\n",
    "* *Policy-based*: identify best policy directly\n",
    "* *Value-based*: estimate value of a decision\n",
    "* *Off-policy*: can learn from historic data from other agent\n",
    "* *On-policy*: requires active learning from current decisions\n",
    "\n",
    "## Duelling Deep Q Networks for Reinforcement Learning\n",
    "\n",
    "Q = The expected future rewards discounted over time. This is what we are trying to maximise.\n",
    "\n",
    "The aim is to teach a network to take the current state observations and recommend the action with greatest Q.\n",
    "\n",
    "Duelling is very similar to Double DQN, except that the policy net splits into two. One component reduces to a single value, which will model the state *value*. The other component models the *advantage*, the difference in Q between different actions (the mean value is subtracted from all values, so that the advtantage always sums to zero). These are aggregated to produce Q for each action. \n",
    "\n",
    "<img src=\"./images/duelling_dqn.png\" width=\"500\"/>\n",
    "\n",
    "Q is learned through the Bellman equation, where the Q of any state and action is the immediate reward achieved + the discounted maximum Q value (the best action taken) of next best action, where gamma is the discount rate.\n",
    "\n",
    "$$Q(s,a)=r + \\gamma.maxQ(s',a')$$\n",
    "\n",
    "## Key DQN components\n",
    "\n",
    "<img src=\"./images/dqn_components.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "## General method for Q learning:\n",
    "\n",
    "Overall aim is to create a neural network that predicts Q. Improvement comes from improved accuracy in predicting 'current' understood Q, and in revealing more about Q as knowledge is gained (some rewards only discovered after time).\n",
    "\n",
    "<img src=\"./images/dqn_process.png\" width=\"600|\"/>\n",
    "    \n",
    "Target networks are used to stabilise models, and are only updated at intervals. Changes to Q values may lead to changes in closely related states (i.e. states close to the one we are in at the time) and as the network tries to correct for errors it can become unstable and suddenly lose signficiant performance. Target networks (e.g. to assess Q) are updated only infrequently (or gradually), so do not have this instability problem.\n",
    "\n",
    "## Training networks\n",
    "\n",
    "Double DQN contains two networks. This ammendment, from simple DQN, is to decouple training of Q for current state and target Q derived from next state which are closely correlated when comparing input features.\n",
    "\n",
    "The *policy network* is used to select action (action with best predicted Q) when playing the game.\n",
    "\n",
    "When training, the predicted best *action* (best predicted Q) is taken from the *policy network*, but the *policy network* is updated using the predicted Q value of the next state from the *target network* (which is updated from the policy network less frequently). So, when training, the action is selected using Q values from the *policy network*, but the the *policy network* is updated to better predict the Q value of that action from the *target network*. The *policy network* is copied across to the *target network* every *n* steps (e.g. 1000).\n",
    "\n",
    "<img src=\"./images/dqn_training.png\" width=\"700|\"/>\n",
    "\n",
    "## Bagging (Bootstrap Aggregation)\n",
    "\n",
    "Each network is trained from the same memory, but have different starting weights and are trained on different bootstrap samples from that memory. In this example actions are chosen randomly from each of the networks (an alternative could be to take the most common action recommended by the networks, or an average output). This bagging method may also be used to have some measure of uncertainty of action by looking at the distribution of actions recommended from the different nets. Bagging may also be used to aid exploration during stages where networks are providing different suggested action.\n",
    "\n",
    "<img src=\"./images/bagging.png\" width=\"800|\"/>\n",
    "\n",
    "## Noisy layers\n",
    "\n",
    "Noisy layers are an alternative to epsilon-greedy exploration (here, we leave the epsilon-greedy code in the model, but set it to reduce to zero immediately after the period of fully random action choice).\n",
    "\n",
    "For every weight in the layer we have a random value that we draw from the normal distribution. This random value is used to add noise to the output. The parameters for the extent of noise for each weight, sigma, are stored within the layer and get trained as part of the standard back-propogation.\n",
    "\n",
    "A modification to normal nosiy layers is to use layers with ‘factorized gaussian noise’. This reduces the number of random numbers to be sampled (so is less computationally expensive). There are two random vectors, one with the size of the input, and the other with the size of the output. A random matrix is created by calculating the outer product of the two vectors.\n",
    "\n",
    "## Prioritised replay\n",
    "\n",
    "In standard DQN samples are taken randomly from the memory (replay buffer). In *prioritised replay* samples are taken in proportion to their loss when training the network; where the network has the greatest error in predicting the target valur of a state/action, then those samples will be sampled more frequently (which will reduce the error in the network until the sample is not prioritised). In other words, the training focuses more heavenly on samples it gets most wrong, and spends less time training on samples that it can acurately predict already.\n",
    "\n",
    "This priority may also be used as a weight for training the network, but this i snot implemented here; we use loss just for sampling.\n",
    "\n",
    "When we use the loss for priority we add a small value (1e-5) t the loss. This avoids any sample having zero priority (and never having a chance of being sampled). For frequency of sampling we also raise the loss to the power of 'alpha' (default value of 0.6). Smaller values of alpha will compress the differences between samples, making the priority weighting less significant in the frequency of sampling.\n",
    "\n",
    "The memory stores the priority/loss of state/action/Next_state/reward, and this is particular to each network, so we create a separate memory for each network.\n",
    "\n",
    "## References\n",
    "\n",
    "Double DQN: \n",
    "van Hasselt H, Guez A, Silver D. (2015) Deep Reinforcement Learning with Double Q-learning. arXiv:150906461 http://arxiv.org/abs/1509.06461\n",
    "\n",
    "Bagging:\n",
    "Osband I, Blundell C, Pritzel A, et al. (2016) Deep Exploration via Bootstrapped DQN. arXiv:160204621 http://arxiv.org/abs/1602.04621\n",
    "\n",
    "Noisy networks:\n",
    "Fortunato M, Azar MG, Piot B, et al. (2019) Noisy Networks for Exploration. arXiv:170610295 http://arxiv.org/abs/1706.10295\n",
    "\n",
    "Prioritised replay:\n",
    "Schaul T, Quan J, Antonoglou I, et al (2016). Prioritized Experience Replay. arXiv:151105952 http://arxiv.org/abs/1511.05952\n",
    "\n",
    "\n",
    "Code for the nosiy layers comes from:\n",
    "\n",
    "Lapan, M. (2020). Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition. Packt Publishing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code structure\n",
    "\n",
    "<img src=\"./images/dqn_program_structure.png\" width=\"700|\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           1 Import packages                                  #\n",
    "################################################################################\n",
    "\n",
    "from amboworld.environment import Env\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Use a double ended queue (deque) for memory\n",
    "# When memory is full, this will replace the oldest value with the new one\n",
    "from collections import deque\n",
    "\n",
    "# Supress all warnings (e.g. deprecation warnings) for regular use\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           2 Define model parameters                          #\n",
    "################################################################################\n",
    "\n",
    "# Set whether to display on screen (slows model)\n",
    "DISPLAY_ON_SCREEN = False\n",
    "# Discount rate of future rewards\n",
    "GAMMA = 0.99\n",
    "# Learing rate for neural network\n",
    "LEARNING_RATE = 0.003\n",
    "# Maximum number of game steps (state, action, reward, next state) to keep\n",
    "MEMORY_SIZE = 10000000\n",
    "# Sample batch size for policy network update\n",
    "BATCH_SIZE = 5\n",
    "# Number of game steps to play before starting training (all random actions)\n",
    "REPLAY_START_SIZE = 50000\n",
    "# Number of steps between policy -> target network update\n",
    "SYNC_TARGET_STEPS = 1000\n",
    "# Exploration rate (epsilon) is probability of choosing a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.0\n",
    "# Reduction in epsilon with each game step\n",
    "EXPLORATION_DECAY = 0.0\n",
    "# Training episodes\n",
    "TRAINING_EPISODES = 50\n",
    "\n",
    "# Set number of parallel networks\n",
    "NUMBER_OF_NETS = 5\n",
    "# Results filename\n",
    "RESULTS_NAME = 'bagging_pr_noisy_d3qn'\n",
    "\n",
    "# SIM PARAMETERS\n",
    "RANDOM_SEED = 42\n",
    "SIM_DURATION = 5000\n",
    "NUMBER_AMBULANCES = 9\n",
    "NUMBER_INCIDENT_POINTS = 3\n",
    "INCIDENT_RADIUS = 2\n",
    "NUMBER_DISPTACH_POINTS = 25\n",
    "AMBOWORLD_SIZE = 50\n",
    "INCIDENT_INTERVAL = 20\n",
    "EPOCHS = 2\n",
    "AMBO_SPEED = 60\n",
    "AMBO_FREE_FROM_HOSPITAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                 3 Define DQN (Duelling Deep Q Network) class                 #\n",
    "#                    (Used for both policy and target nets)                    #\n",
    "################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Code for nosiy layers comes from:\n",
    "\n",
    "Lapan, M. (2020). Deep Reinforcement Learning Hands-On: Apply modern RL methods \n",
    "to practical problems of chatbots, robotics, discrete optimization, \n",
    "web automation, and more, 2nd Edition. Packt Publishing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    Noisy layer for network.\n",
    "    \n",
    "    For every weight in the layer we have a random value that we draw from the\n",
    "    normal distribution.Paraemters for the noise, sigma, are stored within the\n",
    "    layer and get trained as part of the standard back-propogation.\n",
    "    \n",
    "    'register_buffer' is used to create tensors in the network that are not\n",
    "    updated during back-propogation. They are used to create normal \n",
    "    distributions to add noise (multiplied by sigma which is a paramater in the\n",
    "    network).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features,\n",
    "                 sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(\n",
    "            in_features, out_features, bias=bias)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self.sigma_weight = nn.Parameter(w)\n",
    "        z = torch.zeros(out_features, in_features)\n",
    "        self.register_buffer(\"epsilon_weight\", z)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self.sigma_bias = nn.Parameter(w)\n",
    "            z = torch.zeros(out_features)\n",
    "            self.register_buffer(\"epsilon_bias\", z)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * \\\n",
    "                   self.epsilon_bias.data\n",
    "        v = self.sigma_weight * self.epsilon_weight.data + self.weight\n",
    "        return F.linear(input, v, bias)\n",
    "    \n",
    "\n",
    "class NoisyFactorizedLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    NoisyNet layer with factorized gaussian noise. This reduces the number of\n",
    "    random numbers to be sampled (so less computationally expensive). There are \n",
    "    two random vectors. One with the size of the input, and the other with the \n",
    "    size of the output. A random matrix is create by calculating the outer \n",
    "    product of the two vectors.\n",
    "    \n",
    "    'register_buffer' is used to create tensors in the network that are not\n",
    "    updated during back-propogation. They are used to create normal \n",
    "    distributions to add noise (multiplied by sigma which is a paramater in the\n",
    "    network).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 sigma_zero=0.4, bias=True):\n",
    "        super(NoisyFactorizedLinear, self).__init__(\n",
    "            in_features, out_features, bias=bias)\n",
    "        sigma_init = sigma_zero / math.sqrt(in_features)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self.sigma_weight = nn.Parameter(w)\n",
    "        z1 = torch.zeros(1, in_features)\n",
    "        self.register_buffer(\"epsilon_input\", z1)\n",
    "        z2 = torch.zeros(out_features, 1)\n",
    "        self.register_buffer(\"epsilon_output\", z2)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self.sigma_bias = nn.Parameter(w)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_input.normal_()\n",
    "        self.epsilon_output.normal_()\n",
    "\n",
    "        func = lambda x: torch.sign(x) * torch.sqrt(torch.abs(x))\n",
    "        eps_in = func(self.epsilon_input.data)\n",
    "        eps_out = func(self.epsilon_output.data)\n",
    "\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias + self.sigma_bias * eps_out.t()\n",
    "        noise_v = torch.mul(eps_in, eps_out)\n",
    "        v = self.weight + self.sigma_weight * noise_v\n",
    "        return F.linear(input, v, bias)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"Deep Q Network. Udes for both policy (action) and target (Q) networks.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        \"\"\"Constructor method. Set up neural nets.\"\"\"\n",
    "        \n",
    "        # nerurones per hidden layer = 2 * max of observations or actions\n",
    "        neurons_per_layer = 2 * max(observation_space, action_space)\n",
    "\n",
    "        # Set starting exploration rate\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # Set up action space (choice of possible actions)\n",
    "        self.action_space = action_space\n",
    "              \n",
    "        \n",
    "        # First layerswill be common to both Advantage and value\n",
    "        super(DQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(observation_space, neurons_per_layer),\n",
    "            nn.ReLU()            \n",
    "            )\n",
    "        \n",
    "        # Advantage has same number of outputs as the action space\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyFactorizedLinear(neurons_per_layer, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            NoisyFactorizedLinear(neurons_per_layer, action_space)\n",
    "            )\n",
    "        \n",
    "        # State value has only one output (one value per state)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, 1)\n",
    "            )        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n",
    "        \n",
    "        # Act randomly if random number < exploration rate\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            # Otherwise get predicted Q values of actions\n",
    "            q_values = self.forward(torch.FloatTensor(state))\n",
    "            # Get index of action with best Q\n",
    "            action = np.argmax(q_values.detach().numpy()[0])\n",
    "        \n",
    "        return  action\n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        action_q = value + advantage - advantage.mean()\n",
    "        return action_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                    4 Define policy net training function                     #\n",
    "################################################################################\n",
    "\n",
    "def optimize(policy_net, target_net, memory):\n",
    "    \"\"\"\n",
    "    Update  model by sampling from memory.\n",
    "    Uses policy network to predict best action (best Q).\n",
    "    Uses target network to provide target of Q for the selected next action.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Do not try to train model if memory is less than reqired batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return    \n",
    " \n",
    "    # Reduce exploration rate (exploration rate is stored in policy net)\n",
    "    policy_net.exploration_rate *= EXPLORATION_DECAY\n",
    "    policy_net.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                      policy_net.exploration_rate)\n",
    "    # Sample a random batch from memory\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    for state, action, reward, state_next, terminal, index in batch:\n",
    "        \n",
    "        state_action_values = policy_net(torch.FloatTensor(state))\n",
    "        \n",
    "        # Get target Q for policy net update\n",
    "       \n",
    "        if not terminal:\n",
    "            # For non-terminal actions get Q from policy net\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach next state values from gradients to prevent updates\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Get next state action with best Q from the policy net (double DQN)\n",
    "            policy_next_state_values = policy_net(torch.FloatTensor(state_next))\n",
    "            policy_next_state_values = policy_next_state_values.detach()\n",
    "            best_action = np.argmax(policy_next_state_values[0].numpy())\n",
    "            # Get target net next state\n",
    "            next_state_action_values = target_net(torch.FloatTensor(state_next))\n",
    "            # Use detach again to prevent target net gradients being updated\n",
    "            next_state_action_values = next_state_action_values.detach()\n",
    "            best_next_q = next_state_action_values[0][best_action].numpy()\n",
    "            updated_q = reward + (GAMMA * best_next_q)      \n",
    "            expected_state_action_values[0][action] = updated_q\n",
    "        else:\n",
    "            # For termal actions Q = reward (-1)\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach values from gradients to prevent gradient update\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_state_action_values[0] = reward\n",
    " \n",
    "        # Set net to training mode\n",
    "        policy_net.train()\n",
    "        # Reset net gradients\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        # calculate loss\n",
    "        loss_v = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "        # Backpropogate loss\n",
    "        loss_v.backward()\n",
    "        # Update replay buffer (add 1e-5 to loss to avoid zero priority with no\n",
    "        # chance of being sampled).\n",
    "        loss_numpy = loss_v.data.numpy()\n",
    "        memory.update_priorities(index, loss_numpy + 1e-5)\n",
    "        # Update network gradients\n",
    "        policy_net.optimizer.step()  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                  5 Define prioritised replay memory class                    #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class NaivePrioritizedBuffer():\n",
    "    \"\"\"\n",
    "    Based on code from https://github.com/higgsfield/RL-Adventure\n",
    "    \n",
    "    Each sample (state, action, reward, next_state, done) has an associated\n",
    "    priority, which is the loss from training the policy network. The priority\n",
    "    is used to adjust the frequency of sampling.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=MEMORY_SIZE, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add sample (state, action, reward, next_state, done) to memory, or\n",
    "        replace oldest sample if memory full\"\"\"\n",
    "\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            # Add new sample when room in memory\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            # Replace sample when memory full\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # Set maximum priority present\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        # Increment replacement position\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        # Get priorities\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        # Raise priorities by the square of 'alpha' \n",
    "        # (lower alpha compresses differences)\n",
    "        probs  = prios ** self.prob_alpha\n",
    "        \n",
    "        # Normlaise priorities\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Sample using priorities for relative sampling frequency\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # Add index to sample (used to update priority after getting new loss)\n",
    "        batch = []        \n",
    "        for index, sample in enumerate(samples):\n",
    "            sample = list(sample)\n",
    "            sample.append(indices[index])\n",
    "            batch.append(sample)\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def update_priorities(self, index, priority):\n",
    "        \"\"\"Update sample priority with new loss\"\"\"\n",
    "        self.priorities[index] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                       6  Define results plotting function                    #\n",
    "################################################################################\n",
    "\n",
    "def plot_results(run, exploration, score, mean_call_to_arrival, \n",
    "                 mean_assignment_to_arrival):\n",
    "    \"\"\"Plot and report results at end of run\"\"\"\n",
    "\n",
    "    # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot results\n",
    "    lns1 = ax1.plot(\n",
    "        run, exploration, label='exploration', color='g', linestyle=':')\n",
    "\n",
    "    lns2 = ax2.plot(run, mean_call_to_arrival,\n",
    "             label='call to arrival', color='r')\n",
    "    lns3 = ax2.plot(run, mean_assignment_to_arrival,\n",
    "             label='assignment to arrival', color='b', linestyle='--')\n",
    "\n",
    "    # Get combined legend\n",
    "    lns = lns1 + lns2 + lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "        \n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('exploration')\n",
    "    ax2.set_ylabel('Response time')\n",
    "    filename = 'output/' + RESULTS_NAME +'.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                 7 Main program                               #\n",
    "################################################################################\n",
    "\n",
    "def qambo():\n",
    "    \"\"\"Main program loop\"\"\"\n",
    "\n",
    "    ############################################################################\n",
    "    #                          8 Set up environment                            #\n",
    "    ############################################################################\n",
    "\n",
    "    # Set up game environemnt\n",
    "    sim = Env(\n",
    "        random_seed = RANDOM_SEED,\n",
    "        duration_incidents = SIM_DURATION,\n",
    "        number_ambulances = NUMBER_AMBULANCES,\n",
    "        number_incident_points = NUMBER_INCIDENT_POINTS,\n",
    "        incident_interval = INCIDENT_INTERVAL,\n",
    "        number_epochs = EPOCHS,\n",
    "        number_dispatch_points = NUMBER_DISPTACH_POINTS,\n",
    "        incident_range = INCIDENT_RADIUS,\n",
    "        max_size = AMBOWORLD_SIZE,\n",
    "        ambo_kph = AMBO_SPEED,\n",
    "        ambo_free_from_hospital = AMBO_FREE_FROM_HOSPITAL\n",
    "    )\n",
    "\n",
    "    # Get number of observations returned for state\n",
    "    observation_space = sim.observation_size\n",
    "\n",
    "    # Get number of actions possible\n",
    "    action_space = sim.action_number\n",
    "\n",
    "    ############################################################################\n",
    "    #                    9 Set up policy and target nets                       #\n",
    "    ############################################################################\n",
    "\n",
    "    # Set up policy and target neural nets\n",
    "    policy_nets = [DQN(observation_space, action_space)\n",
    "                   for i in range(NUMBER_OF_NETS)]\n",
    "    target_nets = [DQN(observation_space, action_space)\n",
    "                   for i in range(NUMBER_OF_NETS)]\n",
    "    best_nets = [DQN(observation_space, action_space)\n",
    "                   for i in range(NUMBER_OF_NETS)]\n",
    "    \n",
    "    # Set optimizer, copy weights from policy_net to target, and \n",
    "    for i in range(NUMBER_OF_NETS):\n",
    "        # Set optimizer\n",
    "        policy_nets[i].optimizer = optim.Adam(\n",
    "            params=policy_nets[i].parameters(), lr=LEARNING_RATE)\n",
    "        # Copy weights from policy -> target\n",
    "        target_nets[i].load_state_dict(policy_nets[i].state_dict())\n",
    "        # Set target net to eval rather than training mode\n",
    "        target_nets[i].eval() \n",
    "\n",
    "    ############################################################################\n",
    "    #                            10 Set up memory                              #\n",
    "    ############################################################################\n",
    "\n",
    "    # Set up memomry\n",
    "    memory = [NaivePrioritizedBuffer() for i in range(NUMBER_OF_NETS)]\n",
    "\n",
    "    ############################################################################\n",
    "    #                     11 Set up + start training loop                      #\n",
    "    ############################################################################\n",
    "\n",
    "    # Set up run counter and learning loop\n",
    "    run = 0\n",
    "    all_steps = 0\n",
    "    continue_learning = True\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    # Set up list for results\n",
    "    results_run = []\n",
    "    results_exploration = []\n",
    "    results_score = []\n",
    "    results_mean_call_to_arrival = []\n",
    "    results_mean_assignment_to_arrival = []\n",
    "\n",
    "    # Continue repeating games (episodes) until target complete\n",
    "    while continue_learning:\n",
    "\n",
    "        ########################################################################\n",
    "        #                           12 Play episode                            #\n",
    "        ########################################################################\n",
    "\n",
    "        # Increment run (episode) counter\n",
    "        run += 1\n",
    "\n",
    "        ########################################################################\n",
    "        #                             13 Reset game                            #\n",
    "        ########################################################################\n",
    "\n",
    "        # Reset game environment and get first state observations\n",
    "        state = sim.reset()\n",
    "\n",
    "        # Reset total reward and rewards list\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "\n",
    "        # Reshape state into 2D array with state obsverations as first 'row'\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "\n",
    "        # Continue loop until episode complete\n",
    "        while True:\n",
    "\n",
    "            ####################################################################\n",
    "            #                       14 Game episode loop                       #\n",
    "            ####################################################################\n",
    "\n",
    "            ####################################################################\n",
    "            #                       15 Get action                              #\n",
    "            ####################################################################\n",
    "\n",
    "            # Get actions to take (use evalulation mode)\n",
    "            actions = []\n",
    "            for i in range(NUMBER_OF_NETS):\n",
    "                policy_nets[i].eval()\n",
    "                actions.append(policy_nets[i].act(state))\n",
    "                \n",
    "            # Randomly choose an action from net actions\n",
    "            random_index = random.randint(0, NUMBER_OF_NETS - 1)\n",
    "            action = actions[random_index]   \n",
    "\n",
    "            ####################################################################\n",
    "            #                 16 Play action (get S', R, T)                    #\n",
    "            ####################################################################\n",
    "\n",
    "            # Act\n",
    "            state_next, reward, terminal, info = sim.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update trackers\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Reshape state into 2D array with state observations as first 'row'\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "\n",
    "            # Update display if needed\n",
    "            if DISPLAY_ON_SCREEN:\n",
    "                sim.render()\n",
    "\n",
    "            ####################################################################\n",
    "            #                  17 Add S/A/R/S/T to memory                      #\n",
    "            ####################################################################\n",
    "\n",
    "            # Record state, action, reward, new state & terminal\n",
    "            for i in range(NUMBER_OF_NETS):\n",
    "                memory[i].remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            # Update state\n",
    "            state = state_next\n",
    "\n",
    "            ####################################################################\n",
    "            #                  18 Check for end of episode                     #\n",
    "            ####################################################################\n",
    "\n",
    "            # Actions to take if end of game episode\n",
    "            if terminal:\n",
    "                # Get exploration rate\n",
    "                exploration = policy_nets[0].exploration_rate\n",
    "                # Clear print row content\n",
    "                clear_row = '\\r' + ' ' * 79 + '\\r'\n",
    "                print(clear_row, end='')\n",
    "                print(f'Run: {run}, ', end='')\n",
    "                print(f'Exploration: {exploration: .3f}, ', end='')\n",
    "                average_reward = np.mean(rewards)\n",
    "                print(f'Average reward: {average_reward:4.1f}, ', end='')\n",
    "                mean_assignment_to_arrival = np.mean(info['assignment_to_arrival'])\n",
    "                print(f'Mean assignment to arrival: {mean_assignment_to_arrival:4.1f}, ', end='')\n",
    "                mean_call_to_arrival = np.mean(info['call_to_arrival'])\n",
    "                print(f'Mean call to arrival: {mean_call_to_arrival:4.1f}, ', end='')\n",
    "                demand_met = info['fraction_demand_met']\n",
    "                print(f'Demand met {demand_met:0.3f}')\n",
    "\n",
    "                # Add to results lists\n",
    "                results_run.append(run)\n",
    "                results_exploration.append(exploration)\n",
    "                results_score.append(total_reward)\n",
    "                results_mean_call_to_arrival.append(mean_call_to_arrival)\n",
    "                results_mean_assignment_to_arrival.append(mean_assignment_to_arrival)\n",
    "\n",
    "                # Save model if best reward\n",
    "                total_reward = np.sum(rewards)\n",
    "                if total_reward > best_reward:\n",
    "                    best_reward = total_reward\n",
    "                    # Copy weights to best net\n",
    "                    for i in range(NUMBER_OF_NETS):\n",
    "                        best_nets[i].load_state_dict(policy_nets[i].state_dict())\n",
    "\n",
    "                ################################################################\n",
    "                #             18b Check for end of learning                    #\n",
    "                ################################################################\n",
    "\n",
    "                if run == TRAINING_EPISODES:\n",
    "                    continue_learning = False\n",
    "\n",
    "                # End episode loop\n",
    "                break\n",
    "\n",
    "            ####################################################################\n",
    "            #                        19 Update policy net                      #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Avoid training model if memory is not of sufficient length\n",
    "            if len(memory[0]) > REPLAY_START_SIZE:\n",
    "        \n",
    "                # Update policy net\n",
    "                for i in range(NUMBER_OF_NETS):\n",
    "                    optimize(policy_nets[i], target_nets[i], memory[i])\n",
    "\n",
    "                ################################################################\n",
    "                #             20 Update target net periodically                #\n",
    "                ################################################################\n",
    "                \n",
    "                # Use load_state_dict method to copy weights from policy net\n",
    "                if all_steps % SYNC_TARGET_STEPS == 0:\n",
    "                    for i in range(NUMBER_OF_NETS):\n",
    "                        target_nets[i].load_state_dict(\n",
    "                            policy_nets[i].state_dict())\n",
    "\n",
    "    ############################################################################\n",
    "    #             21 Learning complete - plot and save results                 #\n",
    "    ############################################################################\n",
    "\n",
    "    # Target reached. Plot results\n",
    "    plot_results(results_run, results_exploration, results_score,\n",
    "                 results_mean_call_to_arrival, results_mean_assignment_to_arrival)\n",
    "    \n",
    "    # SAVE RESULTS \n",
    "    run_details = pd.DataFrame()\n",
    "    run_details['run'] = results_run\n",
    "    run_details['exploration '] = results_exploration\n",
    "    run_details['mean_call_to_arrival'] = results_mean_call_to_arrival\n",
    "    run_details['mean_assignment_to_arrival'] = results_mean_assignment_to_arrival\n",
    "    filename = 'output/' + RESULTS_NAME + '.csv'\n",
    "    run_details.to_csv(filename, index=False)\n",
    "\n",
    "    ############################################################################\n",
    "    #                             Test best model                              #\n",
    "    ############################################################################\n",
    "    \n",
    "    print()\n",
    "    print('Test Model')\n",
    "    print('----------')\n",
    "    \n",
    "    for i in range(NUMBER_OF_NETS):\n",
    "        best_nets[i].eval()\n",
    "        best_nets[i].exploration_rate = 0\n",
    "\n",
    "    # Set up results dictionary\n",
    "    results = dict()\n",
    "    results['call_to_arrival'] = []\n",
    "    results['assign_to_arrival'] = []\n",
    "    results['demand_met'] = []\n",
    "\n",
    "    # Replicate model runs\n",
    "    for run in range(30):\n",
    "\n",
    "        # Reset game environment and get first state observations\n",
    "        state = sim.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "\n",
    "        # Continue loop until episode complete\n",
    "        while True:\n",
    "            # Get actions to take (use evalulation mode)\n",
    "            actions = []\n",
    "            for i in range(NUMBER_OF_NETS):\n",
    "                actions.append(best_nets[i].act(state))\n",
    "                \n",
    "            # Randomly choose an action from net actions\n",
    "            random_index = random.randint(0, NUMBER_OF_NETS - 1)\n",
    "            action = actions[random_index]   \n",
    "            \n",
    "            # Act\n",
    "            state_next, reward, terminal, info = sim.step(action)\n",
    "            # Reshape state into 2D array with state observations as first 'row'\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            # Update state\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print(f'Run: {run}, ', end='')\n",
    "                mean_assignment_to_arrival = np.mean(info['assignment_to_arrival'])\n",
    "                print(f'Mean assignment to arrival: {mean_assignment_to_arrival:4.1f}, ', end='')\n",
    "                mean_call_to_arrival = np.mean(info['call_to_arrival'])\n",
    "                print(f'Mean call to arrival: {mean_call_to_arrival:4.1f}, ', end='')\n",
    "                demand_met = info['fraction_demand_met']\n",
    "                print(f'Demand met: {demand_met:0.3f}')\n",
    "\n",
    "                # Add to results\n",
    "                results['call_to_arrival'].append(mean_call_to_arrival)\n",
    "                results['assign_to_arrival'].append(mean_assignment_to_arrival)\n",
    "                results['demand_met'].append(demand_met)\n",
    "                \n",
    "                # End episode loop\n",
    "                break\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    filename = './output/results_' + RESULTS_NAME +'.csv'\n",
    "    results.to_csv(filename, index=False)\n",
    "    print()\n",
    "    print(results.describe())    \n",
    "\n",
    "    return run_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, Exploration:  1.000, Average reward: -448.4, Mean assignment to arrival: 18.5, Mean call to arrival: 19.0, Demand met 0.999\n",
      "Run: 2, Exploration:  1.000, Average reward: -445.4, Mean assignment to arrival: 18.5, Mean call to arrival: 19.0, Demand met 0.999\n",
      "Run: 3, Exploration:  1.000, Average reward: -452.9, Mean assignment to arrival: 18.7, Mean call to arrival: 19.2, Demand met 1.000\n",
      "Run: 4, Exploration:  1.000, Average reward: -444.5, Mean assignment to arrival: 18.5, Mean call to arrival: 19.0, Demand met 1.000\n",
      "Run: 5, Exploration:  1.000, Average reward: -444.5, Mean assignment to arrival: 18.5, Mean call to arrival: 19.0, Demand met 1.000\n",
      "Run: 6, Exploration:  1.000, Average reward: -441.4, Mean assignment to arrival: 18.4, Mean call to arrival: 18.9, Demand met 1.000\n",
      "Run: 7, Exploration:  1.000, Average reward: -439.4, Mean assignment to arrival: 18.4, Mean call to arrival: 18.9, Demand met 1.000\n",
      "Run: 8, Exploration:  1.000, Average reward: -455.3, Mean assignment to arrival: 18.7, Mean call to arrival: 19.2, Demand met 1.000\n",
      "Run: 9, Exploration:  1.000, Average reward: -439.4, Mean assignment to arrival: 18.4, Mean call to arrival: 18.9, Demand met 1.000\n",
      "Run: 10, Exploration:  1.000, Average reward: -446.3, Mean assignment to arrival: 18.5, Mean call to arrival: 19.0, Demand met 1.000\n",
      "Run: 11, Exploration:  0.000, Average reward: -281.1, Mean assignment to arrival: 14.8, Mean call to arrival: 15.3, Demand met 1.000\n",
      "Run: 12, Exploration:  0.000, Average reward: -212.2, Mean assignment to arrival: 12.4, Mean call to arrival: 12.9, Demand met 1.000\n",
      "Run: 13, Exploration:  0.000, Average reward: -168.1, Mean assignment to arrival: 11.1, Mean call to arrival: 11.6, Demand met 1.000\n",
      "Run: 14, Exploration:  0.000, Average reward: -207.6, Mean assignment to arrival: 12.3, Mean call to arrival: 12.8, Demand met 1.000\n",
      "Run: 15, Exploration:  0.000, Average reward: -176.3, Mean assignment to arrival: 11.2, Mean call to arrival: 11.7, Demand met 1.000\n",
      "Run: 16, Exploration:  0.000, Average reward: -177.4, Mean assignment to arrival: 11.7, Mean call to arrival: 12.2, Demand met 1.000\n",
      "Run: 17, Exploration:  0.000, Average reward: -181.6, Mean assignment to arrival: 11.6, Mean call to arrival: 12.1, Demand met 1.000\n",
      "Run: 18, Exploration:  0.000, Average reward: -263.6, Mean assignment to arrival: 13.2, Mean call to arrival: 13.7, Demand met 1.000\n",
      "Run: 19, Exploration:  0.000, Average reward: -392.0, Mean assignment to arrival: 16.7, Mean call to arrival: 17.2, Demand met 1.000\n",
      "Run: 20, Exploration:  0.000, Average reward: -399.1, Mean assignment to arrival: 17.0, Mean call to arrival: 17.5, Demand met 1.000\n",
      "Run: 21, Exploration:  0.000, Average reward: -490.4, Mean assignment to arrival: 18.6, Mean call to arrival: 19.2, Demand met 1.000\n",
      "Run: 22, Exploration:  0.000, Average reward: -582.9, Mean assignment to arrival: 20.7, Mean call to arrival: 21.2, Demand met 1.000\n",
      "Run: 23, Exploration:  0.000, Average reward: -582.7, Mean assignment to arrival: 20.6, Mean call to arrival: 21.1, Demand met 1.000\n",
      "Run: 24, Exploration:  0.000, Average reward: -581.3, Mean assignment to arrival: 20.6, Mean call to arrival: 21.1, Demand met 1.000\n",
      "Run: 25, Exploration:  0.000, Average reward: -580.7, Mean assignment to arrival: 20.6, Mean call to arrival: 21.1, Demand met 1.000\n",
      "Run: 26, Exploration:  0.000, Average reward: -603.0, Mean assignment to arrival: 21.1, Mean call to arrival: 21.6, Demand met 1.000\n",
      "Run: 27, Exploration:  0.000, Average reward: -581.4, Mean assignment to arrival: 20.5, Mean call to arrival: 21.0, Demand met 1.000\n",
      "Run: 28, Exploration:  0.000, Average reward: -574.8, Mean assignment to arrival: 20.4, Mean call to arrival: 20.9, Demand met 1.000\n",
      "Run: 29, Exploration:  0.000, Average reward: -607.2, Mean assignment to arrival: 21.0, Mean call to arrival: 21.5, Demand met 1.000\n",
      "Run: 30, Exploration:  0.000, Average reward: -670.8, Mean assignment to arrival: 22.4, Mean call to arrival: 22.9, Demand met 1.000\n",
      "Run: 31, Exploration:  0.000, Average reward: -591.6, Mean assignment to arrival: 20.7, Mean call to arrival: 21.2, Demand met 1.000\n",
      "Run: 32, Exploration:  0.000, Average reward: -608.0, Mean assignment to arrival: 21.3, Mean call to arrival: 21.9, Demand met 1.000\n",
      "Run: 33, Exploration:  0.000, Average reward: -575.1, Mean assignment to arrival: 20.5, Mean call to arrival: 21.1, Demand met 1.000\n",
      "Run: 34, Exploration:  0.000, Average reward: -599.4, Mean assignment to arrival: 21.0, Mean call to arrival: 21.5, Demand met 1.000\n",
      "Run: 35, Exploration:  0.000, Average reward: -582.5, Mean assignment to arrival: 20.7, Mean call to arrival: 21.2, Demand met 1.000\n",
      "Run: 36, Exploration:  0.000, Average reward: -586.4, Mean assignment to arrival: 20.8, Mean call to arrival: 21.4, Demand met 1.000\n",
      "Run: 37, Exploration:  0.000, Average reward: -593.9, Mean assignment to arrival: 20.9, Mean call to arrival: 21.4, Demand met 0.999\n",
      "Run: 38, Exploration:  0.000, Average reward: -592.4, Mean assignment to arrival: 21.0, Mean call to arrival: 21.5, Demand met 1.000\n",
      "Run: 39, Exploration:  0.000, Average reward: -614.2, Mean assignment to arrival: 21.6, Mean call to arrival: 22.1, Demand met 1.000\n",
      "Run: 40, Exploration:  0.000, Average reward: -617.5, Mean assignment to arrival: 21.8, Mean call to arrival: 22.3, Demand met 1.000\n",
      "Run: 41, Exploration:  0.000, Average reward: -591.8, Mean assignment to arrival: 21.5, Mean call to arrival: 22.0, Demand met 1.000\n",
      "Run: 42, Exploration:  0.000, Average reward: -612.4, Mean assignment to arrival: 21.9, Mean call to arrival: 22.4, Demand met 1.000\n",
      "Run: 43, Exploration:  0.000, Average reward: -621.4, Mean assignment to arrival: 22.1, Mean call to arrival: 22.6, Demand met 1.000\n",
      "Run: 44, Exploration:  0.000, Average reward: -643.1, Mean assignment to arrival: 22.4, Mean call to arrival: 22.9, Demand met 1.000\n",
      "Run: 45, Exploration:  0.000, Average reward: -628.4, Mean assignment to arrival: 22.2, Mean call to arrival: 22.7, Demand met 1.000\n",
      "Run: 46, Exploration:  0.000, Average reward: -646.1, Mean assignment to arrival: 22.6, Mean call to arrival: 23.1, Demand met 0.999\n",
      "Run: 47, Exploration:  0.000, Average reward: -618.6, Mean assignment to arrival: 22.1, Mean call to arrival: 22.6, Demand met 1.000\n",
      "Run: 48, Exploration:  0.000, Average reward: -578.7, Mean assignment to arrival: 21.3, Mean call to arrival: 21.9, Demand met 1.000\n",
      "Run: 49, Exploration:  0.000, Average reward: -494.6, Mean assignment to arrival: 20.3, Mean call to arrival: 20.8, Demand met 1.000\n",
      "Run: 50, Exploration:  0.000, Average reward: -486.8, Mean assignment to arrival: 20.0, Mean call to arrival: 20.5, Demand met 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Model\n",
      "----------\n",
      "Run: 0, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 1, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 2, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 3, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 0.999\n",
      "Run: 4, Mean assignment to arrival: 10.3, Mean call to arrival: 10.7, Demand met: 1.000\n",
      "Run: 5, Mean assignment to arrival: 10.6, Mean call to arrival: 11.1, Demand met: 1.000\n",
      "Run: 6, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 1.000\n",
      "Run: 7, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "Run: 8, Mean assignment to arrival: 10.8, Mean call to arrival: 11.3, Demand met: 1.000\n",
      "Run: 9, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 1.000\n",
      "Run: 10, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 0.999\n",
      "Run: 11, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "Run: 12, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 1.000\n",
      "Run: 13, Mean assignment to arrival: 10.1, Mean call to arrival: 10.6, Demand met: 1.000\n",
      "Run: 14, Mean assignment to arrival: 10.1, Mean call to arrival: 10.6, Demand met: 1.000\n",
      "Run: 15, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "Run: 16, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 17, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "Run: 18, Mean assignment to arrival: 10.8, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 19, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 20, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 21, Mean assignment to arrival: 10.6, Mean call to arrival: 11.1, Demand met: 1.000\n",
      "Run: 22, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 0.999\n",
      "Run: 23, Mean assignment to arrival: 10.9, Mean call to arrival: 11.4, Demand met: 1.000\n",
      "Run: 24, Mean assignment to arrival: 10.3, Mean call to arrival: 10.8, Demand met: 1.000\n",
      "Run: 25, Mean assignment to arrival: 10.1, Mean call to arrival: 10.6, Demand met: 1.000\n",
      "Run: 26, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "Run: 27, Mean assignment to arrival: 10.6, Mean call to arrival: 11.1, Demand met: 1.000\n",
      "Run: 28, Mean assignment to arrival: 10.7, Mean call to arrival: 11.2, Demand met: 1.000\n",
      "Run: 29, Mean assignment to arrival: 10.5, Mean call to arrival: 11.0, Demand met: 1.000\n",
      "\n",
      "       call_to_arrival  assign_to_arrival  demand_met\n",
      "count        30.000000          30.000000   30.000000\n",
      "mean         11.003269          10.501726    0.999900\n",
      "std           0.227756           0.227231    0.000305\n",
      "min          10.569309          10.069018    0.999000\n",
      "25%          10.813300          10.316452    1.000000\n",
      "50%          11.032046          10.531628    1.000000\n",
      "75%          11.206357          10.700785    1.000000\n",
      "max          11.404373          10.898400    1.000000\n"
     ]
    }
   ],
   "source": [
    "######################## MODEL ENTRY POINT #####################################\n",
    "\n",
    "# Run model and return last run results\n",
    "last_run = qambo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
